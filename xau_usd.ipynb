{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas_ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_ta as ta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2024 = pd.read_csv('df_2024.csv')\n",
    "df_all = pd.read_csv('df_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2024['timestamp'] = pd.to_datetime(df_2024['timestamp'])\n",
    "df_all['timestamp'] = pd.to_datetime(df_all['timestamp'])\n",
    "\n",
    "df_all = df_all[df_all['timestamp'] < '2024-01-01 00:00:00']\n",
    "df_combined = pd.concat([df_all, df_2024], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    # ----- 1) Timestamp handling\n",
    "    if 'timestamp' in df.columns:\n",
    "        ts = pd.to_datetime(df['timestamp'], utc=True, errors='coerce')\n",
    "        df = df.set_index(ts)           # keep index tz-aware\n",
    "    elif not isinstance(df.index, pd.DatetimeIndex):\n",
    "        raise ValueError(\"No datetime index or column found\")\n",
    "\n",
    "    df.drop(columns=['timestamp'], errors='ignore', inplace=True)\n",
    "\n",
    "    # ----- 2) Candle statistics\n",
    "    rng = df['high'] - df['low']\n",
    "    body = (df['close'] - df['open']).abs()\n",
    "    df['body_ratio']        = body.div(rng).replace([np.inf, -np.inf], np.nan)\n",
    "    df['upper_wick_ratio']  = (df['high'] - df[['open', 'close']].max(axis=1)).div(rng)\n",
    "    df['lower_wick_ratio']  = (df[['open', 'close']].min(axis=1) - df['low']).div(rng)\n",
    "\n",
    "    df['day_of_week'] = df.index.day_name()\n",
    "\n",
    "    return df.dropna(subset=['body_ratio', 'upper_wick_ratio', 'lower_wick_ratio'])\n",
    "\n",
    "def add_indicators(df, price='close'):\n",
    "    df = df.copy()\n",
    "\n",
    "    # -------- Trend structure\n",
    "    df['ema_9']   = df[price].ewm(span=9,  adjust=False).mean()\n",
    "    df['ema_21']  = df[price].ewm(span=21, adjust=False).mean()\n",
    "    df['ema_50']  = df[price].ewm(span=50, adjust=False).mean()\n",
    "    df['ema_100']  = df[price].ewm(span=100, adjust=False).mean()\n",
    "    df['ema_200'] = df[price].ewm(span=200,adjust=False).mean()   # long-term trend filter\n",
    "\n",
    "    # -------- Momentum / volatility\n",
    "    macd = ta.macd(df[price], fast=12, slow=26, signal=9)\n",
    "    macd.columns = ['macd', 'macd_signal', 'macd_hist']\n",
    "    df = df.join(macd)\n",
    "\n",
    "    df['rsi_14']  = ta.rsi(df[price], length=14)\n",
    "    df['atr_14']  = ta.atr(df.high, df.low, df.close, length=14)\n",
    "    df['adx_14']  = ta.adx(df.high, df.low, df.close, length=14)['ADX_14']  # trend strength\n",
    "    bb = ta.bbands(df[price], length=20, std=2)\n",
    "    df = df.join(bb)   # upper, middle, lower bands\n",
    "\n",
    "    # -------- Boolean helpers (vectorised)\n",
    "    df['bull_9_21'] = (df['ema_9'] > df['ema_21']).astype(int)\n",
    "    df['bull_21_50']= (df['ema_21']> df['ema_50']).astype(int)\n",
    "    df['bull_50_100'] = (df['ema_50'] > df['ema_100']).astype(int)\n",
    "    df['bull_100_200']= (df['ema_100']> df['ema_200']).astype(int)\n",
    "    df['above_200'] = (df[price]   > df['ema_200']).astype(int)\n",
    "    df['macd_cross_up']   = ((df['macd_hist'] > 0) & (df['macd_hist'].shift() <= 0)).astype(int)\n",
    "    df['macd_cross_down'] = ((df['macd_hist'] < 0) & (df['macd_hist'].shift() >= 0)).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "df_combined.dropna(inplace = True)\n",
    "\n",
    "df_cleaned = preprocess(df_combined)\n",
    "df_indicators = add_indicators(df_cleaned)\n",
    "\n",
    "df_indicators.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Trade Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trade_labels(df, window=20, time_limit=36, max_R=2):\n",
    "    \n",
    "    \"\"\"\n",
    "    Generate multi-label TP/SL labels for an XAU/USD dataset, assuming only BUY trades.\n",
    "    Adds:\n",
    "      - TP1_hit, TP2_hit, TP3_hit, SL_hit (binary)\n",
    "      - entry_price: the open price of each candle\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.copy()\n",
    "\n",
    "    # 1) Entry price is the open of the candle\n",
    "    df['entry_price'] = df['open']\n",
    "\n",
    "    # 2) Compute the past swing low (shifted to avoid lookahead)\n",
    "    swing_low = df['low'].rolling(window).min().shift(1)\n",
    "\n",
    "    # 3) Calculate R = distance from entry down to that swing low\n",
    "    df['R'] = (df['entry_price'] - swing_low).abs()\n",
    "\n",
    "    df['R'] = df['R'].apply(lambda x: min(x, max_R))  # Cap R at 2\n",
    "\n",
    "    # 4) SL and TP levels (all BUY trades)\n",
    "    df['SL_price'] = df['entry_price'] - df['R']\n",
    "    df['TP1']      = df['entry_price'] + 0.5 * df['R']\n",
    "    df['TP2']      = df['entry_price'] + 1 * df['R']\n",
    "    df['TP3']      = df['entry_price'] + 1.5 * df['R']\n",
    "\n",
    "    # 5) Initialize label columns\n",
    "    df['TP1_hit'] = 0\n",
    "    df['TP2_hit'] = 0\n",
    "    df['TP3_hit'] = 0\n",
    "    df['SL_hit']  = 0\n",
    "\n",
    "    # 6) Evaluate hits in the next `time_limit` bars\n",
    "    for idx in range(len(df)):\n",
    "        entry_idx = df.index[idx]\n",
    "        sl  = df.at[entry_idx, 'SL_price']\n",
    "        tp1 = df.at[entry_idx, 'TP1']\n",
    "        tp2 = df.at[entry_idx, 'TP2']\n",
    "        tp3 = df.at[entry_idx, 'TP3']\n",
    "\n",
    "        future = df.iloc[idx + 1 : idx + 1 + time_limit]\n",
    "\n",
    "        hit_TP1 = hit_TP2 = hit_TP3 = hit_SL = 0\n",
    "\n",
    "        for _, row in future.iterrows():\n",
    "           \n",
    "            if row['low'] <= sl:\n",
    "                hit_SL = 1\n",
    "                break\n",
    "                \n",
    "            elif row['high'] >= tp3:\n",
    "                hit_TP1 = hit_TP2 = hit_TP3 = 1\n",
    "                break\n",
    "                \n",
    "            elif row['high'] >= tp2:\n",
    "                hit_TP1 = hit_TP2 = 1\n",
    "                break\n",
    "                \n",
    "            elif row['high'] >= tp1:\n",
    "                hit_TP1 = 1\n",
    "                break\n",
    "\n",
    "        # Assign to df\n",
    "        df.at[entry_idx, 'TP1_hit'] = hit_TP1\n",
    "        df.at[entry_idx, 'TP2_hit'] = hit_TP2\n",
    "        df.at[entry_idx, 'TP3_hit'] = hit_TP3\n",
    "        df.at[entry_idx, 'SL_hit']  = hit_SL\n",
    "\n",
    "    return df\n",
    "\n",
    "df_labels = generate_trade_labels(df_indicators)\n",
    "df_labels.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings, json, numpy as np, pandas as pd, joblib, catboost as cb\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.experimental import enable_halving_search_cv        # noqa: F401\n",
    "from sklearn.model_selection import TimeSeriesSplit, HalvingGridSearchCV\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# ---------- 1. DATA & FEATURE ENGINEERING -------------------------------\n",
    "\n",
    "df = df_labels.copy()\n",
    "df.sort_values(\"timestamp\", inplace=True)\n",
    "\n",
    "# basic returns & volatility\n",
    "df[\"ret_1h\"]  = df[\"close\"].pct_change(1)\n",
    "df[\"ret_6h\"]  = df[\"close\"].pct_change(6)\n",
    "df[\"ret_24h\"] = df[\"close\"].pct_change(24)\n",
    "df[\"vol_12\"]  = df[\"close\"].pct_change().rolling(12).std()\n",
    "\n",
    "le_day_of_week = LabelEncoder()\n",
    "df[\"day_of_week\"] = le_day_of_week.fit_transform(df[\"day_of_week\"])\n",
    "\n",
    "# Save the encoder\n",
    "joblib.dump(le_day_of_week, 'label_encoder_day_of_week.pkl')\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# ---------- 2.  FEATURES / LABELS ---------------------------------------\n",
    "label_cols  = [\"TP1_hit\", \"TP2_hit\", \"TP3_hit\"]\n",
    "feature_cols = sorted(set(df.columns) - set(label_cols) - {\"timestamp\"} - {\"SL_hit\"})\n",
    "\n",
    "print(feature_cols)\n",
    "\n",
    "X, y = df[feature_cols], df[label_cols].astype(int)\n",
    "\n",
    "split = int(len(df)*0.80)\n",
    "X_train, X_test = X.iloc[:split], X.iloc[split:]\n",
    "y_train, y_test = y.iloc[:split], y.iloc[split:]\n",
    "\n",
    "# ---------- 3.  CLASS WEIGHTS -------------------------------------------\n",
    "class_wt = {\n",
    "    c: compute_class_weight(\n",
    "            class_weight='balanced',         # â† kw-arg\n",
    "            classes=[0, 1],                  # â† kw-arg\n",
    "            y=y_train[c]                     # â† kw-arg\n",
    "        )\n",
    "    for c in label_cols\n",
    "}\n",
    "\n",
    "# ---------- 4.  CatBoost factory ----------------------------------------\n",
    "def make_cat(spw):\n",
    "    return cb.CatBoostClassifier(\n",
    "        loss_function   = \"Logloss\",\n",
    "        eval_metric     = \"F1\",\n",
    "        task_type       = \"GPU\",\n",
    "        devices         = \"0\",\n",
    "        iterations      = 4000,\n",
    "        learning_rate   = 0.03,\n",
    "        depth           = 6,\n",
    "        l2_leaf_reg     = spw[0]/spw[1],\n",
    "        random_seed     = 42,\n",
    "        verbose         = False\n",
    "    )\n",
    "\n",
    "param_grid = {\n",
    "    \"learning_rate\": [0.02, 0.05],\n",
    "    \"depth\"        : [4, 6, 8],\n",
    "    \"bagging_temperature\": [0.5, 1.0],\n",
    "    \"l2_leaf_reg\"  : [1, 5, 10]\n",
    "}\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "best_models = {}\n",
    "best_params = {}  # Dictionary to store the best parameters for each label\n",
    "\n",
    "for col in label_cols:\n",
    "    print(f\"\\nðŸ”  Optimising label: {col}\")\n",
    "    cat = make_cat(class_wt[col])\n",
    "\n",
    "    search = HalvingGridSearchCV(\n",
    "        estimator      = cat,\n",
    "        param_grid     = param_grid,\n",
    "        resource       = \"iterations\",\n",
    "        max_resources  = 4000,\n",
    "        min_resources  = 400,\n",
    "        factor         = 3,\n",
    "        scoring        = \"f1\",\n",
    "        cv             = tscv,\n",
    "        n_jobs         = 1,\n",
    "        verbose        = 1,\n",
    "        refit          = True,\n",
    "    )\n",
    "\n",
    "    # â˜… single eval_set (GPU requirement)\n",
    "    search.fit(\n",
    "        X_train, y_train[col],\n",
    "        eval_set=(X_test, y_test[col]),\n",
    "        early_stopping_rounds=100\n",
    "    )\n",
    "\n",
    "    best_models[col] = search.best_estimator_\n",
    "    best_params[col] = search.best_params_  # Save the best parameters\n",
    "    print(\"   best CV-F1 :\", search.best_score_)\n",
    "\n",
    "# ---------- 5. Set a Fixed Threshold of 0.5 ----------------------------------\n",
    "best_thresh = {col: 0.5 for col in label_cols}\n",
    "\n",
    "# ---------- 6. Report ---------------------------------------------------\n",
    "y_pred = pd.DataFrame({\n",
    "    c: (best_models[c].predict_proba(X_test)[:,1] >= 0.5).astype(int)\n",
    "    for c in label_cols\n",
    "}, index=y_test.index)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“Š  HOLD-OUT PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(\n",
    "    y_test, y_pred,\n",
    "    target_names=label_cols,\n",
    "    digits=4, zero_division=0\n",
    "))\n",
    "\n",
    "# ------------------------------------------ 7. Save artefacts ------------------------------------------\n",
    "for c in label_cols:\n",
    "    joblib.dump(best_models[c], f\"catboost_{c}.pkl\")\n",
    "\n",
    "# Save the best parameters from the grid search\n",
    "with open(\"catboost_best_params.json\", \"w\") as fp:\n",
    "    json.dump(best_params, fp, indent=2)\n",
    "\n",
    "with open(\"catboost_thresholds.json\", \"w\") as fp:\n",
    "    json.dump(best_thresh, fp, indent=2)\n",
    "\n",
    "# Save label encoder\n",
    "joblib.dump(le_day_of_week, 'label_encoder_day_of_week.pkl')\n",
    "\n",
    "print(\"\\nâœ…  Saved models, thresholds, best parameters, and label encoder.\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7593605,
     "sourceId": 12071908,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
